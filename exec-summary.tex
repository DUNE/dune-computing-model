\section{Executive Summary}
The Computing Model lays out the general design of DUNE computing infrastructure as well as planning and
organization of its software development effort. It aims to guide the evolution of the DUNE computing
platform in the direction optimal for achieving the scientific goals of the experiment, and will also serve
to inform funding agencies about the scope of work, resource requirements and rate of progress of 
DUNE Software and Computing sector.

After assessment of the characteristics of the data to be generated by the DUNE detector subsystems,
and different data collection modes, it is recognized that DUNE will have to deal with \textit{uniquely diverse
types of experimental data} which sets it apart from many other experiments (such as collider experiments). 
For example, the following data streams that will be generated by the DUNE detectors have been identified
as having distinct characteristics:
%(with a possible conclustion that they should be collected and processed separately):
\begin{itemize}

\item Data due to interactions of beam neutrinos, cosmic muons and to nucleon decay candidate events in the Far Detector

\item Far Detector data due to candidate Supernova Burst (SNB) events

\item The Near Detector Systems (NDS) data

\end{itemize}

\noindent
The first item reflects some of the highest science priorities in DUNE, yet at the same time it is expected
to produce a very modest amount of data, and at a low rate.

The second item (SNB) necessitates collection of a few candidates
events per year, since the potential rate of real Supernova events, efficiency of the neutrino signal detection
and precise background characteristics are not well known at this point. Thorough understanding of these factors will be crucial
in improvement and fine tuning of the DAQ algorithms necessary for establishing the SNB trigger condition.
Due to necessarily long readout window (in tens of seconds), low thresholds for zero suppression (or even
eschewing zero suppression for better efficiency), and the large number of channels in the TPC this type of events
has a clear potential to dominate the total data volume in DUNE. Since the Supernova Burst candidates are not correlated
in any way with the first item (apart from calibrations that can be done independently), there are no pressing reasons to keep
these data streams together during data transfer and processing.

As far as the Near Detector is concerned, it is massive and expected to produce a sizeable amount of data. It is worth mentioning
that there is no event-level correlation between the Far and Near Detectors. The logistics for moving the data to storage and into processing stage
will also be different for this type of data. Since the Near Detector has a more complex structure than the Far Detector, it is
possible that it will require more processing resources for event reconstruction than the Far Detector.

In summary, the scale of storage requirements in DUNE will be set according to the strategy chosen for
collection of the Supernova Burst candidate events data. The scale of processing (e.g. CPU requirements)
will be set by the data coming out of the Near Detector Systems. While precise reconstruction of the
\textit{beam neutrino} interactions will be an important challenge in DUNE, in relative terms it will take less storage
and CPU than the other two items.

%It will be optimal to keep the corresponding data streams (which are not directly related to each other) and associated processing
%separate in order to ensure flexible and more agile data placement and efficient use of available CPU power.

