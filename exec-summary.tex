\section{Executive Summary}
The Computing Model lays out the general design of DUNE computing infrastructure as well as planning and
organization of its software development effort. It aims to guide the evolution of the DUNE computing
platform in the direction optimal for achieving the scientific goals of the experiment, and will also serve
to inform funding agencies about the scope of work, resource requirements and rate of progress of 
DUNE Software and Computing sector.

After assessment of the characteristics of the data to be generated by the DUNE detector subsystems,
and different data collection modes, it is recognized that DUNE will have to deal with uniquely diverse
types of experimental data which sets it apart from many other experiments (such as collider experiments). More specifically,
the following data streams can be identified that are optimally collected and processed separately:

\begin{itemize}

\item Data due to interactions of beam neutrinos, cosmic muons and to nucleon decay candidate events in the Far Detector

\item Data due to candidate Supernova Burst (SNB) events

\item The Near Detector Systems (NDS) data

\end{itemize}

The first item corresponds to some of the highest priorities in DUNE, and at the same time it is expected to produce
a very modest amount of data, and at a low rate.

The second item (SNB) necessitates collection of a few candidates
events per year, since both the potential rate of real Supernova events, efficiency of the neutrino signal detection
and background are not precisely known at this point. Thorough understanding of these factors will be crucial
in improvement and fine tuning of the DAQ algorithms necessary for establishing the SNB trigger condition.
Due to necessarily long readout window (in tens of seconds), low thresholds for zero suppression (or switching
off zero suppression for better efficiency), and the large number of channels in the TPC this type of events
has a clear potential to dominate the total data volume in DUNE. Since it's not correlated in any way with the
first item (apart from calibrations that can be done independently), there are no reasons to keep these
data streams together.

Further, the Near Detector is massive and is expected to produce a sizeable amount of data. At the same time,
the data to be collected with it is essentially due to separate interaction (i.e. there is no event-specific correlation
between the Far and Near Detectors). The logistics for moving the data to storage and into processing stage
will also be different for this type of data.

In summary, the scale of storage requirements in DUNE will be set according to the strategy chosen for
collection of the Supernova Burst candidate events data. The scale of processing (e.g. CPU requirements)
will be set by the data coming out of the Near Detector Systems. While precise reconstruction of the
beam neutrino interaction will be an important challenge in DUNE, in relative terms it will take less storage
and CPU than the other two items.

It will be optimal to keep the corresponding data streams (which are not directly related to each other) and associated processing
separate in order to ensure flexible and more agile data placement and efficient use of available CPU power.

