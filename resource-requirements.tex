\section{Resource Requirements}
\label{sec:resource-requirements}
\subsection{Overview}
The computing requirements of the DUNE experiment will evolve significantly between the time of writing
and the comissioning of the full set of DUNE detector systems, for the following reasons:
\begin{itemize}
\item In the short and medium terms, active work will continue on DUNE prototypes, the ``35t prototype'' and protoDUNE
(see Sections~\ref{sec:35t} and \ref{sec:protodune} correspondingly).

\item The amount of R\&D work for the Near Detector is significant and it can be expected to ramp up as offline software
is being prototyped and extensive simulations will be required.

\item Investigation of optimal reconstruction methods for LArTPC is ongoing and far from complete. This means
that there will be fluctuating resource requirements (mainly CPU) for MC and reconstruction in the course of R\&D
as the algorithms are improved over the years.

\end{itemize}

\noindent
As many hardware and software components in DUNE are under active development,
and the final data model has not yet been created, there are necessarily large uncertainties in the resource requirement estimates
at the time of writing. It is however helpful to review current understanding of the general scale of number so as to provide some
guidance for DUNE resource need going forward.

In the following, ``best effort'' estimates are presented for resources required by the DUNE prototype work,  R\&D for the final DUNE
and ballpart values for what DUNE will need at the start of operations.

\subsection{Prototypes}
\subsubsection{35t}
Estimates for resource requirements for the 35t prototype are given in \ref{sec:35t-resource-requirements}.
In summary, the requirements are:
\begin{itemize}

\item Raw data: $\sim$200\,TB, to be stored permanently on tape

\item Storage needed to support reconstruction: $\sim$400\,TB

%\item Raw data committed to permanent tape storage: $\sim$100\,TB

\item Storage needed for Monte Carlo samples: $\sim$100\,TB

\item CPU for reconstruction: $\sim4\times10^6$\,hours scaled to a ``standard'' Fermigrid node. This translates into $\sim$500 Worker Nodes
committed over a period of one year in order to get the data processed during that time.

\end{itemize}

\subsubsection{protoDUNE/NP04}
Eastimates of protoDUNE storage requirements are fiven in \ref{sec:protodune-zs} and \ref{sec:protodune-dataprocess}.
In summary, the requirements are:
\begin{itemize}

\item Raw data: $\sim$500\,TB, to be stored permanently on tape

%\item Raw data committed to permanent tape storage: $\sim$500\,TB

\item Disk space for staging data at CERN before transmission: $\sim$100\,TB

\item Storage space at FNAL to support reconstruction: $\sim$1\,PB

\item MC data on disk: $\sim$100\,TB

\item CPU for reconstruction: $\sim$O(1000) Worker Nodes
committed over a period of one year in order to get the data processed during that time.

\end{itemize}

\subsection{DUNE}
\subsubsection{Data Volume and Storage Requirements}
Projections for various sources of data in DUNE and associated data volumes are given in
Table \ref{tab:summary-data-table} with comments provided in \ref{summary-annual-volume}.
Under assumptions given throughout Section \ref{sec:data-characteristics} and further detailed
in \ref{sec:data-rate-and-volume-estimates}, the synopsis is as follows:
\begin{itemize}

\item Recording Supernova Burst candidate events and preserving them for detailed analysis (which is necessary to take
full advantage of this unique capability in DUNE) will result in excess of $\sim$500\,TB of raw data annually, under the assumption
of 12 false positives per year. This is the largest single source of raw data in DUNE.

\item The Near Detector Systems are the second largest source of data with initial estimates of $\sim$100\,TB of raw data annualy.

\item Beam neutrino events, cosmic muon events and proton decay events combined are expected to produce
much less raw data than the leading sources, on the scale of  $\sim$5\,TB. Possible implications of this fact
are discussed in \ref{sec:implications-of-data-estimates} on page \pageref{sec:implications-of-data-estimates}.

\item MC data: $\sim$40\,TB (annual).

\item Ratio of derived data to its source (whether its raw or MC) is assumed to be 2 based on extrapolation from other experiments.

\item An additional factor of 4 (the ``headroom'') is required to support reprocessing of the data (such as with improved calibrations etc)
and running two concurrent releases of DUNE software.

\end{itemize}

In summary, the estimated total storage requirement is estimated as $\sim$6\,PB (annual).

\subsubsection{CPU Requirements}
At the time of writing, Monte Carlo studies in DUNE result the following CPU utliziation pattern is observed according to Grid monitoring data:
\begin{itemize}

\item Far Detector MC: 6 campaigns per year, 40\,k CPU*hrs each, total $\sim$240\,k\,CPU*hrs

\item Near Detector MC: 3 campaigns per year, 40\,k CPU*hrs each, total $\sim$120\,k\,CPU*hrs

\item Beam Simulations MC: 3 campaigns per year, 200\,k CPU*hrs each, total $\sim$600\,k\,CPU*hrs

\item MARS group MC: same scale as Beam Simulations, i.e. $\sim$600\,k\,CPU*hrs

\end{itemize}

\noindent
The total CPU budget for Monte Carlo is therefore approximately 1.5\,million CPU hours per year and this number
should be considered as the basis for extrapolation going forward.

Situation with CPU power for reconstruction is much less clear due to the fact that this software has not yet been fully
developed at the time of writing and only rough prototypes exit. Judging from the event rates presented in this document,
reconstruction of the Near Detector will be a larger challenge than processing beam-$\nu$ and cosmic-$\mu$ events due
to relative complexity of the ND design and operating principle, and to an appreciable event rate
(see \ref{sec:fd-data-volume-summary} and \ref{sec:nds-event-rates}). Accepting the nominal ND event rate to be 50\,Hz,
and assuming a $\sim$1\,min per event reconstruction time purely as an order of magnitude estimate device, it can be estimated
that a few thousand Worker Nodes will need to be dedicated to the Near Detector data production, just to keep up with the nominal
data rate. This leads to a rough estimate of at least 30\,million\,CPU*hrs annually.

Reconstruction of the Supernova Burst candidate events presents more challenges with data logistics (due to the very large size
of the readout package) than with reconstruction since the total number of potential neutrinos in a single event is still O(10$^2$).
Currently the concusion is that the resulting CPU requirements will be substantially less than those for the Near Detector.




