\section{Software and Computing Requirements}
\label{sec:requirements}
\subsection{Overview}
\subsubsection{Purpose, Origin and Scope}

These \textit{Requirements} are in essence rules and guidelines for setting policies,
putting in place practices (for example related to data handling) and making technology choices in DUNE.
They describe approach employed by DUNE to issues such as  code management, Grid and Cloud capability, data management etc.
As such  the \textit{Requirements} form the foundation of the DUNE Computing Model. They do not however cover specific \textit{functional requirements}
for the various physics tools software or specific parameters (storage capacity, bandwidth, CPU hours etc) of DUNE computing,
which are to be addressed in the Computing Model proper.

An effort was made to maintain a relatively high-level view of the DUNE computing issues. In cases where it was impossible to establish concrete metrics or
parameters for a specific requirement, it is still listed as an item to be addressed in the future.


%Information necessary for the creation of these Requirements have been collected during a number of DUNE Software and Computing meetings, conference calls
%and extensive information %exchange via e-mail and other means. It is recognized that the Requirements themselves (and the Computing Model) may
%evolve with time, in order to correctly reflect the status of rapidly
%evolving technologies and the DUNE organization itself. Such expectation is reinforced by actual experience of large scale HEP collaborations.
%We anticipate therefore that the requirements will be revised roughly in the middle of the time period between their creation and the commissioning
% of the experiment, and help evolve the DUNE Computing model such that it continues to meet the needs of the Collaboration.

%Certain  requirements need to be specified by a collaboration including the S\&C Organization and other software organizations in DUNE
%(i.e. the Physics Tools Group, the Online/DAQ groups etc). In these cases, the S\&C Organization will work with the appropriate group to insure
%that the requirement addresses the needs of all necessary organizations.  This includes interfacing with various DUNE Project groups in order
%to assure the seamless cooperation between the online and offline areas.

\subsubsection{Structure of the Requirements}

The \textit{Requirements} are organized as a set of categories reflecting major responsibilities of the DUNE Software working group.  Each category will generally
contain information including:

\begin{itemize}
\item \textbf{Description:} Description and scope of the category.
\item \textbf{Definitions:} If necessary, specific definitions of certain items are given.
\item \textbf{Issues:} Statement of issues that requirements in this particular category are expected to address.
\item \textbf{Requirements:} The requirements themselves, formatted as assertions.
\end{itemize}

Where necessary, each of these items may be placed in an individual subcategory to better reflect details and granularity of information being presented. A requirement section may be accompanied by one or more of the additional items:

\begin{itemize}
\item \textbf{Use cases}: Descriptions of any use cases that drive the choice for the requirement.
\item \textbf{Justifications}: Explanations as to why the requirement was chosen.
\item \textbf{Accepted risks}: Any known problems with the requirement which are considered acceptable.
\item \textbf{Alternatives}: Any alternatives that were considered with reasons for rejection.
\item \textbf{Deviations}: Recognized cases where deviations do not violate the intention.
\end{itemize}

In addition, in cases when it helps clarify the scope of a given  requirement, appropriate statements will be made as to what is \textbf{not} required in the context of a particular section.


%%% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\newpage
\subsection{Data Requirements}
\label{sec:dunedata}

\subsubsection{Description}
This section contains the requirements related to handling a variety of data by the DUNE Collaboration.
This includes data produced by DUNE primary detectors and monitoring systems, as well as derived (processed)
datasets and other collections of data -- and where necessary, the metadata associated with these items.
Databases and related technologies will be covered separately in Section~\ref{sec:dunedb}.

The principal requirement for data distribution and access methods is that they need to support a coordinated and widely (indeed globally) distributed network of computing centers and research groups located at DUNE member institutions. Processing of the data is described in Section~\ref{sec:dunedc} (Distributed Computing).

Data Retention policies will aim to provide cost-effective and optimal schedule of data distribution, replication and retirement, in order to maximize
efficiencies in achieving the scientific objectives of DUNE. On a longer time scale, there is a need to put in place policies and procedures for data preservation.
The principal difference between retention and preservation is that the former concerns itself with optimizing ongoing analyses and other types of data processing,
whereas the latter addresses the long term storage and documenting and preserving methods of accessing these data (formats, algorithms, application code etc).


\subsubsection{Definitions}
\label{sec:req-data-definitions}
\begin{itemize}
\item \textbf{Raw Data:} Data saved by a detector (far, near or prototype detector) DAQ or monitoring systems, and information from FNAL beam monitoring.

\item \textbf{Production Software:} the suite of software run to produce an official collaboration result. One example of such software is software used to produce data appearing in a publication. Such software is subject to strict version control, QA and validation, and is utilized in a managed fashion.

\item \textbf{Processed Data:} any data produced by production software (see above) for the collaboration as a whole or to satisfy the needs of working groups.  This includes simulations, data reduction/reconstruction, data skimming and inputs to final analyses. This type of data does not include data samples produced by individual users using software that has not been certified as production software by appropriate Working Groups, Production Managers and if needed by the S\&C Coordinators.

\item \textbf{File catalog:} a general term used to describe a system which performs a range of mapping functions, such as mapping of a Logical File Name (LFN) to one or more physical locations of the file in the distributed data management systems. This functionality is essential for locating physical copies of the data when needed, optimal matching of distributed data to available computing resources, storage accounting and various other aspects of distributed data management. File catalog may also incorporate functionality related to Metadata (see next item).

\item \textbf{Metadata:} data that describes other data.

\item \textbf{Dataset:} a collection of files (potentially in differing formats) and corresponding metadata (uniform across the set) that forms a coherent unit of data used in a computation, and is accounted for as such.
\end{itemize}

\subsubsection{Issues}
The following is a short list of issues that DUNE will need to address in its data handling:
\begin{itemize}
\item Data replication strategy for each major class of data needs to be created.

\item Data retention policies are an important tool to assure cost-effectiveness and overall efficiency of the  data infrastructure.

\item Long-term data preservation policies and procedures need to be properly planned.

\item General rules of access for DUNE collaborators, and for the general public need to be understood in order to assure efficiencies and compliance.

\item Data distribution and access methods will play a major role in resource availability and overall efficiency of resource utilization.

\item The file catalog is one of the central elements of many architectures for handling data, and its performance characteristics are of utmost importance.

\item File and dataset metadata: management of the large and heterogeneous volume of data in DUNE  (e.g. Monte Carlo samples, raw data, processed data in any stage of analysis or transformation etc) requires creation, storage and appropriate use of coherent metadata, which must allow identification, location and retrieval of collections of data necessary for specific purposes. It is crucial that the design of the metadata and any system for its utilization is such that it allows for truly distributed, highly scalable and symmetric strategy of data placement.

\item Loss of information contained in the file catalog and/or metadata system may have pernicious effects such as appearance of ``orphaned" or ``dark" data, i.e. data 
residing in storage that cannot be efficiently located and utilized by the Collaboration while still taking up valuable space.

\item Data design and formats: having a consistent approach to data design and policies to maintain relevant standards and interfaces is crucial for efficient and reliable software development processes and operations of DUNE.

\item Raw data collected by DUNE  online systems (such as DAQ) will be stored (buffered) at a facility located close to the Far Detector, then transmitted to central mass storage via the network. Efficient interface for exchange of data between the online and offline systems needs to be established.

\end{itemize}
\subsubsection{DAQ Data Interface Requirements}
\begin{itemize}
\item Data formats, schemas and other crucial parameters pertaining to data recorded by DUNE  data acquisition systems shall 
be formulated and documented in close cooperation between the Software working group and Online Software/DAQ Group.

\item Handling of the data being recorded by DAQ systems shall become responsibility of the Software working group once such 
data is first deposited into a general purpose mass storage system, having passed necessary validation steps.
% -mxp this needs to be re-written - we'll be harvesting data from DAQ in protoDUNE at an earlier stage.

\end{itemize}

\subsubsection{Replication Requirements}
\label{sec:req-raw-data-replication}
\textit{Raw data replication}

\begin{itemize}
\item Facilities storing official copies of the raw data shall possess adequate storage capacity, availability (i.e. minimal downtime),
network connectivity and bandwidth as well as other relevant infrastructure characteristics to be determined by the Software working group.

\item The number of raw data replicas at any given time shall be sufficient to protect the raw data as a whole from data loss,
by utilizing a variety of techniques such as redundancy inherent in replication, automated error detection, automated data repairs and others.

\item Raw data replicas shall not be required to reside in a single storage facility and may be divided in managed datasets distributed to a few designated DUNE computing centers chosen based on agreements with member institution and taking into account their infrastructure characteristics.

\item Latency of replication of raw data shall be within a 24 hour period, counting from arrival of new data to the initial storage location (buffer), and to the point where transmitted data is deposited in mass storage at the remote location, and validated and accounted for in the data handling system.

\item Any storage system or host selected for the official copy (replica) of the raw data shall employ storage technology with an expected loss of no more than one unit of data per million per year.

\end{itemize}
\textit{Processed data replication}
\begin{itemize}

\item The processed data shall be generated at, and distributed to DUNE computing facilities at participating institutions 
based on a combination of factors such as research interests of the corresponding working group operating at a particular location, 
resource availability and scheduling policies of the Workload Management System employed by DUNE.

\item The number of replicas of the processed data shall not be subject to a fixed minimum. 

\item The number and placement of replicas of the processed data shall be determined dynamically based on requests of a particular processing campaign, with priorities set by collective decisions of the Physics Working Groups and the Software working group.
\end{itemize}

\noindent
\textit{General replication}

\begin{itemize}
\item Mechanisms shall be put in place to assert validity of the data being replicated and/or transmitted.

\item One mechanism for validating data replication shall be checksum controls.

\item Control of data placement, volume, status and other characteristics shall be available.
\end{itemize}

%%% --------------------------------------------------------------------
\subsubsection{Data Retention and Preservation Requirements}
\label{sec:req-raw-data-retention}
\textit{Raw data retention}
\begin{itemize}
\item Raw data shall be stored at least for the duration of the experiment.

\item Exceptions to the raw data retention requirement shall be made by the Collaboration.

\item Any part of raw data shall always be readable by software available to the Collaboration, for the duration of its existence.

\end{itemize}
\noindent
\textit{Processed data retention}
\begin{itemize}
\item Specific policies shall be created by the Software working groups conveners and the
Data Management Coordinators reporting to them, who will be tasked with collecting information
regarding requests for specific data types and datasets/segments of data, monitoring capacity,
access patterns and other relevant data for effective decision-making. 

\item Processed data shall have no fixed retention time.
\end{itemize}
\noindent
\textit{Data preservation}
\label{sec:dunepres}
\begin{itemize}
\item The Software working group shall develop a long-term data preservation strategy in compliance with regulations put in place by the funding agencies and utilizing best practices in science and industry.

\item The funding and support model for long-term DUNE data preservation shall exist by the time of commissioning of the detector, and shall be established in consultation with the funding agencies.
\end{itemize}

%%% --------------------------------------------------------------------
\subsubsection{General Rules of Access to DUNE Data}
\label{sec:rules-of-access-to-data}

\begin{itemize}
\item Access to raw or processed data by official members of DUNE Collaboration  shall not be limited by any specific policies.

\item Technical implementations shall not unduly restrict access by any member of the Collaboration.

\item Each member institution and individual member of  DUNE shall abide by the data access and distribution rules contained in this document.

\item The Software working group shall develop a long-term public data access strategy in compliance with regulations put in place by the funding agencies and utilizing best practices in science and industry.

\item The long-term public data access strategy shall exist by the time of commissioning of the detector, and shall be established in consultation with the funding agencies.
\end{itemize}

%%% --------------------------------------------------------------------
\subsubsection{Data Distribution and Access Methods}
\label{sec:req-data-distribution-and-access}
\textit{Raw Data}

\noindent
The raw data volume and characteristics make it very distinct from other data types, and it is helpful to consider it separately from the processed data in the context of this section. 
\begin{itemize}
\item Raw data shall be distributed to institutions in a managed manner, based on specific requests, in cases not already covered by the raw data replication strategy.

\item Immediate (low latency) access to raw data held in official replicas outside of actively managed production processing streams shall not be guaranteed.
\end{itemize}
\noindent
\textit{Processed Data}

\begin{itemize}
\item A highly symmetrical placement strategy for the processed data shall exist, i.e. in principle both input and output data for any job or application can reside at any site or host which is a part of the DUNE distributed data network.

\item Processed data shall not be required to be copied to a single principal location.

\item A single principal location shall not be relied upon as a sole source of input data.

\item Processed data at any location, regardless of its provenance,  shall be accessible to DUNE users via submitted jobs or applications from any other DUNE site.
\end{itemize}
\noindent
\textit{All Data}
\begin{itemize}
\item Direct access to high capacity, high latency back-end storage e.g. tape, from running jobs or applications, shall not be allowed.

\item When accessed remotely, data shall be made available using standard interfaces and protocols compatible with Grid and Cloud technology as implemented in widely available software stacks (cf. the Open Science Grid).

\item Where practical, sites without extensive local storage (e.g. without the Storage Element capability) shall be made available for deployment of DUNE 
computational payload by utilizing appropriate network-based methods of data retrieval and upload to and from the worker node.

\item In deciding details of data placement strategy, the Software working group shall take into account the infrastructure characteristics
and support level at participating sites, as well as potential legal or policy barriers impacting systems and individuals based on nationality or location.


\end{itemize}

%%% --------------------------------------------------------------------
\subsubsection{File Catalog Requirements}
\label{sec:file-catalog-req}

\begin{itemize}
\item A file catalog system shall be put in place by the DUNE Software working group.


\item The file catalog system shall be protected from data loss to the greatest extent possible, by utilizing redundancy, replication and backup and restore systems.


\item The file catalog system shall have interfaces which are flexible and extensible enough to cover the range of data storage and distribution technologies employed in DUNE.

\item The file catalog system shall cover the totality of distributed storage used by DUNE, i.e. it won't just describe data at a single site, but instead will allow its clients to locate potentially multiple replicas of the data at multiple sites, thus opening optimization possibilities.

\item The file catalog system shall need to have verifiable scalability properties which should allow it to scale up to the necessary level of data volume and throughput, in order to meet the needs of DUNE data processing throughout the lifetime of the experiment.

\item At a minimum, the file catalog system shall incorporate handling of checksum information in order to facilitate detection of errors and sanity checks during operations on data.

\end{itemize}

%%% --------------------------------------------------------------------
\subsubsection{File and Dataset Metadata Requirements}
\label{sec:metadata-req}

\begin{itemize}
\item A metadata system shall be created to support the distributed data processing capabilities of DUNE.

\item The metadata shall cover the data managed by all participating sites, utilizing a variety of middleware and storage media.

\item The metadata system shall not be coupled to a particular storage solution or architecture, so as to allow flexibility and ways to evolve DUNE data storage.

\item Guaranteed scalability of DUNE metadata system shall be assured according to parameters set by Software working group.

\item Information contained in the metadata system shall be protected from data loss to the greatest extent possible, by utilizing redundancy, replication and backup and restore systems.

\end{itemize}

%%% --------------------------------------------------------------------
\subsubsection{Data Design and Format Requirements}

\begin{itemize}
\item Raw data shall be created according to formats which have been explicitly accepted for use by the Collaboration.

\item Processed data shall be created according to formats which have been explicitly accepted for use by the Collaboration.

\item Data formats shall be developed according to specific mandates set by the Software working group.

\item Data formats shall be subject to peer review, before their acceptance.

\item Simplicity of design and straightforward unpacking and access algorithms shall be part of requirements for any data format.

\item The Software working group shall be proactive in evaluation of existing and potential future data formats.

\item The Software working group shall assure that obsolete and/or inefficient formats are phased out on a schedule that is not disruptive to crucial data processing streams.

\item Any collection of data (e.g. contained in a file) shall have methods put in place for identification of the data format and its version.

\item Any collection of data (e.g. contained in a file) shall be self-describing.

\item Any collection of data (e.g. contained in a file) shall contain provenance information.

\item Full backward compatibility for raw data shall be guaranteed at any time, e.g. via maintenance of legacy interfaces, or by bulk conversion to a new format, with subsequent rigorous validation.
\end{itemize}

%%% --------------------------------------------------------------------
\subsubsection{Use cases}
\begin{itemize}
\item A working group wants to reprocess all raw data in order to perform searches for new physics or rare events that would be missed by pre-existing standard analyses.  To accommodate technology changes over time, the Software working group has rewritten very old data into new formats and with each software release has performed validation checks to assure that any changes to the raw data file formats have been accommodated.

\item A research group from a member institution located outside of the United States requested a large portion of raw data to be placed at their facility, in order to run various preliminary analyses. Upon working out the schedule of data movement, the DUNE Data Management  monitors progress of data transmission which is performed using one of the Grid protocols.

\item DUNE secured access to a computing facility which unfortunately does not have significant local storage capability. Utilizing ``data in the Grid'' technology, the Workload Management System makes it possible for the worker nodes at this facility to download input data and upload results to a different location, utilizing high bandwidth networks.

\item A user needs to locate datasets produced in a few Monte Carlo simulation runs, produced under specific conditions, for further analysis. Finding this data is made possible by the Metadata and File Catalog Systems.

\item After a long processing campaign, results of a particular simulation run have been fully analyzed and the campaign declared a success. To conserve storage space, a decision is made to retire the data (i.e. make it eligible for deletion as necessary) since the new simulation wave will use an improved version of geometry, which will be adopted going forward.


\item A user received a request for a subset of DUNE data from an institution that is not a member of DUNE. According to the rules, the user then refers the requestor to the
Software working group and general management of DUNE.
\end{itemize}
%%% --------------------------------------------------------------------
\subsubsection{Justifications}
\begin{itemize}
\item Having geographically distributed multiple replicas of "precious" data is a common and standard way
to ensure it iss not lost during the normal course of operations where occasional localized technical
faults cannot be excluded, and also in case of natural disasters where a single data center may suffer
a sudden power outage, and/or damage and data loss.

\item The File Catalog provides functionality that is very useful (and often crucial) in a number of architectures
of data management systems, which forms the basis for the corresponding requirement.

\item Having the ability to always read raw data no matter when and how it was produced (including the format)
is essential to meet requirements of Section~\ref{sec:dunepres} (data preservation).

\item The value of '24 hours' for raw data replication requirements needs additional justification. 
\end{itemize}

%%% --------------------------------------------------------------------
\subsubsection{Accepted Risks}
\begin{itemize}
\item Having to implement a system such as File Catalog leads to creation of a potential bottleneck
and/or single point of failure. This will need to be countered with rigorous scalability analysis,
robust implementation, backup procedures and adequate operational support.

\item It is likely that at least part of the additional raw data replicas will be stored outside of the United States.

\item Errors in decisions related to data retention will potentially result in necessity to reproduce large datasets.
\end{itemize}
%%% --------------------------------------------------------------------
\subsubsection{Deviations}
\begin{itemize}
\item Due to a hardware malfunction, a software bug or other such factors, a portion of raw data may fail basic validation.
A special decision may be made to retire such data on a short time scale in order to conserve storage space.

\item Once a particular body of data has been completely migrated to a new format, and properly validated,
a special decision may be made to lift the ``backward read capability'' requirement for the software accessing
the data, since the old format is effectively no longer required and the new software has full access to data anyhow.
The data kept in the old format can then be deleted to free up storate space.
\end{itemize}

%%% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\newpage
\subsection{Databases}
\label{sec:dunedb}
\subsubsection{Description}

Databases are a crucial domain in DUNE, providing the foundation for handling experiment conditions and calibration data,
metadata, file catalogs and a variety of information systems.  A large part (but not necessarily all) information held
in the experiment databases is typically derived from the experiment data itself (see  Section~\ref{sec:dunedata}, ``Data'')
and thus shares some of the same requirement concepts.  However, due to specifics of data handling in this domain, databases
are considered distinct enough that their requirements are stated separately.  

\subsubsection{Definitions}
\begin{itemize}

\item \textbf{Database (DB):} A database is a collection of data, organized in a way that supports processes and activities requiring that data.

\item \textbf{RDBMS:} Relational Database Management System.

\item \textbf{Database server:} A database server is a computer program that provides database services to other computer
programs or computers, as defined by the clientâ€“server model. Most of the time the server runs on dedicated hardware,
in which case the term ``server'' equally applies to that hardware as well.

\item \textbf{Slave database:} A database in which information is synchronized to a master database,
which is considered the authoritative source of data.

\item \textbf{Master database:} A database which collects and stores information from various sources
and serves as the source of data in the master/slave model. A replica residing in a slave may in turn
become a master for a different system.

\item \textbf{Database owner:} A collaborator or group of collaborators responsible for configuring,
populating and maintaining a database and making it available to the Collaboration.

\item \textbf{Collaboration database system:} The ensemble of all databases and their servers on which the collaboration relies.

\item \textbf{Application-specific databases:} a database which is not required to be distributed because of its tight
coupling to a specific application. Such databases are exempt from master/slave distribution mechanism as determined
by consensus of the Software working group.

\item \textbf{noSQL:} A collective term referring to a large and diverse group of new database technologies which
are built on principles different from RDBMS and may offer advantages in certain application areas.
\end{itemize}

\subsubsection{Issues}
\begin{itemize}
\item In most physics experiments, RDBMS remains the prevailing technology in use, oftentimes as a legacy platform.
\item There is no single RDBMS standard, there are multiple solutions and variations of the technologies as well as licensing options attached to it, from free to commercial.
\item Commercial database solutions have the advantage of proven performance and well understood scalability, but also are often associated with considerable licensing fees, expensive support and implications for interface design.
\item Data  integrity requires having one definitive DB vs a few receiving same data simultaneously.
\item Replication is one of core features necessary in many usage scenarios.
\item DB backup/restore procedures are needed to protect crucial data.
\item Performance and scalability problems are commonplace in large scale projects.
\item The noSQL technology has taken root in industry, and should be evaluated for use in DUNE.
\item Databases utilized in online systems (such as DAQ) represent a special case, as their performance and availability must be prioritized in order for DUNE to achieve maximum data-taking efficiency.
\item There are multiple ways to access the data residing in the databases, and it needs to be done in a way optimal for each use case.
\end{itemize}

\subsubsection{Requirements}
\label{sec:req-db}
\textit{Master Database}

\begin{itemize}
\item For each specific type of data there shall be only one master database across the Collaboration.

\item Master databases involving multiple servers sharing or copying data among themselves (such as in cases driven by High Availability considerations) shall preserve referential integrity of the data and  have an interface presented to the slave DB servers (and other clients) consistent with that of a single master

\item All master databases shall be available for replication by any appropriately equipped participating computing facility.

\item Replication of master databases shall not have undue performance and/or availability impact on the master DB.

\item Design and implementation for new master databases shall be discussed within DUNE Software working group.  Discussions shall take into
consideration expertise of and ongoing support by the database owners, performance, scalability and long-term sustainability of the database
technology as well as integration with collaboration-wide information systems.
\end{itemize}
\noindent
\textit{Slave Databases}
\begin{itemize}
\item A slave database shall be replicated from a master.

\item A slave database shall not be modified except as part of the official replication process.
\end{itemize}
\noindent
\textit{Backup and restoration}

\begin{itemize}
\item Any master database shall be equipped with a backup mechanism or be otherwise reproducible in the event of a technical failure, 
hardware upgrade and other types data loss.
\item Specific requirements forthe time limits on restoration of each database shall be defined by the S\&C Organization 
and its Database Coordinator on case-by-case basis at a later point, so as to not significantly impact ongoing processing and operations in general.
\end{itemize}
\noindent
\textit{Performance and scalability}

\begin{itemize}
\item Specific metrics defining necessary availability, latency and scalability parameters for each database, such that their use is efficient and does not impede productivity of the Collaboration,  shall be established by the DUNE Software working group.

\item Software working group shall be proactive in monitoring the performance of the Collaboration database systems, identifying bottlenecks and scalability limits.

\item Software working group shall be making plans for data migration, equipment and platform upgrades and any other measures as is 
necessary to provide adequate level of database services to the Collaboration, in close cooperation with the facilities 
and support personnel at each participating site.
\end{itemize}
\noindent
\textit{Technology choices}
\begin{itemize}
\item Database technologies available to the Collaboration shall be considered on merits such as long term cost and 
performance characteristics, available expertise and support levels, available interfaces, scalability and other relevant 
factors such as ease of integration with collaboration-wide information systems.

\item There shall be no fixed requirement to either use or avoid newer technology such as noSQL (or any other) vs RDBMS, 
and such determination will be made on case-by-case basis based on criteria listed above.


\item Decisions pertinent to technology choices shall be made based on discussions within the Software working group.
\end{itemize}
\noindent
\textit{DB Access Methods}

\begin{itemize}

\item The Software working group shall make an concerted effort to ensure that access to DUNE databases it not 
locked into a particular programming language or software package, i.e. flexible and diverse APIs are available.

\item Existing solutions (coming from industry and open source community) shall be given preference over in-house development if other factors are equal.

\item Where possible, direct DB access shall be deprecated in favor of a network service which is to provide features such as modern and flexible security mechanism, data caching, venues for scalability improvements, better resource utilization  and possibilities to replace the database system itself with a different engine, while preserving access methods (APIs).

\item In general, access to databases utilized by the online systems (DAQ etc) shall be limited to these systems (i.e. no external access), in order to ensure stability, performance and availability essential for efficient data-taking.

\item Procedures shall be be put in place to provide copies of the data contained in the online databases, to offline systems and DUNE researchers.

\end{itemize}

\subsubsection{Justifications}
\begin{itemize}

\item Allowing replicas to receive updates from sources other than master DB will lead to loss of data integrity.

\item Having a set of robust backup and restoration procedures is crucial for preserving continuity of DUNE operations and minimizing downtime in cases of technical failures and other disruptive events.

\item It is essential to provide wide access to the data in the master DB, in order to leverage the data itself and the effort put into its maintenance.

\item Optimal choices on specific master database location and technology can not be made without considering details of the application and the wider software and user base of the collaboration.

\item Freedom of innovation and efficiency is gained by allowing flexibility in location and technology choices but 
this may conflict with collaboration wider goals of efficiency, security and consistency, and the right balance needs to be found.

\item Master databases can benefit from geographical proximity to their source of data.

\item Mature database systems provide robust remote replication mechanisms which should not be re-invented by the collaboration.

\item Opening a database connection to the world may lead to a sub-optimal security situation, and this can be mitigated by deploying 
a web service as an insulation layer.

\end{itemize}

\subsubsection{Deviations}
\begin{itemize}

\item Certain application-specific databases, such as a small database used in a small, limited use information 
system (e.g. a specific type of log files of no critical importance, or lookup tables utilized in a simulation, 
which values can be recalculated if needed) shall not be subject to replication requirements.


\item Likewise, certain Web services require databases for their basic functionality, and such database contain information that is not specific or unique to DUNE data, and these cases will also be exempt from the replication requirement.

\item In the interest of guaranteed availability and performance, replication requirements for online systems will be modified (e.g. ongoing automatic replication may be replaced with a managed copying of data, potentially in a different format, to systems that are exposed to users).
\end{itemize}

\subsubsection{Use cases}
\begin{itemize}

\item An application needs a straightforward way to read certain data in the conditions DB even when it is run 
off-line and does not have direct access to the Collaboration database systems. A read-only local replica of the 
data is created, which is immutable and will not be used to feed other systems.

\item Since the performance of the master DB is critically important for it to be able to receive and process 
data at rates commensurate with incoming data, certain types of applications will be restricted to replicas of 
the data located in regional computing centers (as opposed to the master DB itself).

\item A Web service used to provide access to a database incorporates data caching methods which allow the system 
to avoid repetitive queries by serving pre-fetched data, thus greatly improving performance.

\item After accumulating a large volume of Grid job monitoring data, a RDBMS instance started having 
performance problems due to unforeseen scalability issues. Based on decisions by the Software working group, a large portion of data is 
migrated to a noSQL service, thus reducing the load on the principal database.
\end{itemize}
%%% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\newpage
\subsection{Software}
\label{sec:req-software}
\subsubsection{Description}
The requirements in this section cover a variety of software used by the collaboration for producing its scientific results. Emphasis is made on architectural compliance and on managing the development cycle, validation in particular. Requirements in this section do include the software required for collaborative work. Not included are engineering or administrative software.


\subsubsection{Definitions}

\begin{itemize}
\item \textbf{DUNE Offline Software:} The subset of simulation, reconstruction, analysis and production software which is written for the DUNE collaboration.

\item \textbf{External Software:}  Software on which the DUNE Offline Software relies.  Also referred to as ``externals'' or ``third party software''.

\item \textbf{Supported Platform:} A computing platform (OS, compiler and CPU architecture) which the S\&C Infrastructure Group agrees to support.

\item \textbf{User Friendly:} The ability to make use of something with only a reasonably minimal amount of effort by the intended qualified user.

\item \textbf{Revision control:} (also known as version control and source control) is the management of changes to documents, computer programs, web sites, and other collections of information.

\item \textbf{Production Software:} the suite of software run to produce an official collaboration result. One example of such software is software used to produce data appearing in a publication. Such software is subject to strict revision control, QA and validation, and is utilized in a managed fashion.

\end{itemize}

\subsubsection{Issues}
\begin{itemize}

\item To ensure quality and maintainability of DUNE software, emphasis must be put on well defined and optimal architectural patterns.

\item Ensuring architectural compliance is difficult due to wide scope and complexity of DUNE Offline Software. 
Effort needs to be expended to track compliance throughout the software.

\item Revision control and release methodology are critically important tools for managing all stages of software life cycle.

\item CASE and debugging tools boost productivity.

\item Collaborator and public access to repositories and commit privileges should be subject to specific policies.

\item DUNE Offline Software installation procedures must be robust and well documented.

\item Supported platforms must be clearly defined.

\item There are different classes of applications, e.g. laptop or workstation applications vs. production code meant to run on a cluster etc. This affects the methods utilizied in software distribution in each case.

\item Adoption of external tools must be subject of a review process.

\item Adoption of software produced by DUNE collaborators  must be subject of a review process.

\item Software documentation must be adequately maintained.

\item Proprietary software acceptance in DUNE needs proper justification and review.


\end{itemize}

\subsubsection{Software Architecture and Architectural Compliance Requirements}

\begin{itemize}

\item All components of production software shall be created according to appropriately chosen architectural patterns, aimed at maximizing maintainability and adaptability.

\item Basic Architectural Compliance shall be ensured by establishing a code review process and subsequent testing (see Section~\ref{sec:lbnecodeman}), taking into account implementation of components, interfaces and behaviors.

\end{itemize}

\subsubsection{External Software Requirements}
\begin{itemize}
\item Provenance: all third party software available in source form shall be able to be compiled from pristine upstream releases.

\item Provenance: all third party software patches shall be obtained from trusted sources.

\item Provenance: the responsibility for assuring the provenance requirements shall be held by the S\&C Organization.

\item Source code archival: a copy of the source code used to build external software for official releases shall be archived in order to allow for reproducing results at a later time.

\item Reliance on third party binaries: reliance on external software which is available only in binary form shall be kept to an absolute minimum and considered on a case-by-case basis.

\end{itemize}


\subsubsection{Requirements for Supported Platforms and Versions}

\begin{itemize}

\item The S\&C Organization shall develop a policy for supported platforms, based on the needs of the Collaboration.  

\item The supported platform policy shall include the mechanisms by which platforms are selected for support, retired from
 the supported status and what levels of support and methods for its delivery will be provided.
 
\item The S\&C Organization shall determine the supported versions of software, taking into account the computing needs of the Collaboration.
\end{itemize}

\subsubsection{Documentation Requirements}
\begin{itemize}

\item A documentation system shall be selected by the Collaboration and its Software Organization.

\item DUNE Offline Software design, development, commissioning and operation shall be documented using this documentation system.

\end{itemize}


\subsubsection{General Code Management Requirements}
\label{sec:lbnecodeman}
\begin{itemize}
\item The S\&C Organization shall develop and document a set of coding standards to be followed by all DUNE Offline Software.

\item All production software shall be subject to code review.


\item The S\&C Organization and the TAC shall determine additional software which shall be subject to code review.

\item Code reviews shall aim to satisfy the requirements of Architectural Compliance and coding standards.

\item Where warranted, code reviews shall be coordinated with the Physics Tools Group in order to ensure proper functionality and characteristics of the code.


\item Code review feedback and results shall be documented according to general documentation requirements, in order to facilitate access to this information and its usefulness for making corrections.

\item All DUNE Production Software source code shall be maintained in a revision control system.

\item All DUNE Production Software shall be subject to backup and restore procedures to counter possible loss of source code due to technical failures.

\item Contributors to those elements of the Offline Software code base which are not included into the Production Software shall be encouraged to adopt a revision control system as well.

\item Debugging, CASE and other automated tools shall be chosen/accepted by the S\&C Organization for use in various sectors of software development in DUNE.

\item Any code used directly in DUNE production software or representing a dependency for it for shall be preserved (possibly in archived form) for the lifetime of the Collaboration, in order to guarantee its ability to reproduce all and any of its results at any given time.


\item A review process shall be implemented to determine which parts of DUNE Offline Software are no longer required and/or used.

\item DUNE Offline Software that is no longer required and/or used shall be subject to decisions on whether and how to decommission and remove such software in a way that is not disruptive to DUNE and does not contravene other stipulations of these Requirements.

\item A release mechanism shall be put in place, including defined release periods for DUNE Offline Software and externals.

\item A policy on long term software maintenance shall be put in place in order to address eventual rotation of personnel and departure of software developers from the Collaboration.

\item A policy on long term knowledge management shall be put in place in order to address eventual rotation of personnel and departure of software developers from the Collaboration.

\item Software build and installation from source: an installation mechanism based primarily on the source code shall be provided for the DUNE Offline Software and externals which can be run on compatible platforms, 

\item The installation mechanism shall be capable of being run in an automated fashion by collaborators.

\item The installation mechanism shall be documented, including user documentation.

\item An established and documented process shall be established which developers of new DUNE Offline Software packages should follow in order to integrate a package into the greater code base.

\item Software binaries shall be provided in a form that is ready to use (e.g. executable binaries, libraries, properly placed Python modules or scripts).

\item Supplying software binaries shall be the responsibility of the S\&C Organization.

\item A documented installation mechanism shall exist for all supported software binaries.
\end{itemize}


\subsubsection{Validation, Unit Testing and Continuous Integration }
\textit{Definitions}

\begin{itemize}
% -mxp- softened the physics bit
\item \textbf{Validation:} in this context, we define it as software quality control which may include validation of physics functionality.

\item  \textbf{Validation criteria:} a set of characteristics and/or parameters the software must comply with in order to be declared valid (i.e. passed validation).

\item \textbf{Unit Test:} a method by which individual units of source code, sets of one or more computer program modules together with associated control data, usage procedures, and operating procedures are tested to determine if they are fit for use.

\item \textbf{Continuous Integration:} frequent updates of the code in the common repository, combined with running unit tests in an ongoing and coordinated manner.

\item \textbf{Test Harness:} a system supporting the creation and execution of unit tests.


\end{itemize}

\textit{General Validation Requirements}
\begin{itemize}
\item Testing procedures shall be designed in a way which allows them to be run in fully automatic as well as more interactive, user-controlled mode.

\item All of the Production Software used in DUNE must pass validation criteria on all supported platforms.

\item For a given unit version or release, or an external package, to be declared valid, all of its components shall compile in the DUNE environment,
without compiler warnings, set at a level to be defined at a later date.

% \item For an application or program to be declared valid after build procedures are complete, there shall be no errors introduced in the process.

\item Methods shall be provided to reproduce results from prior runs of the same program.


\item Regression test failures shall be thoroughly investigated and result in one of the following:
\begin{itemize}
\item Bugs introduced since the last successful regression test shall be identified and reported.

\item Expected new behavior due to code changes shall result in updating the test suit.

\item New features of the environment, which shall be accounted for, documented and taken into account while updating the test suite.
\end{itemize}

\item The DUNE suite of tests shall test DUNE Offline Software (where necessary) for compliance and compatibility with evolution of data schemas and formats.

\item Validation test results shall be available for viewing by the collaboration.

\item The validation suite shall be tailored for each supported platform to insure it validates what is the approved usage of that platform.

\end{itemize}

\subsubsection{Unit Testing and Continuous Integration}
\begin{itemize}
\item Every DUNE Offline Software package shall provide unit testing capability.

\item There will be a Continuous Integration System (CIS), which shall cover all supported platforms and include necessary test harness capability.

\item The CIS shall rely on the unit tests coming with the packages it covers.

\item Resolution of issues (e.g. bugs) shall be confirmed through unit testing.

\item The CIS and the unit test upon which it relies shall include checks for basic validity of the code (e.g. it doesn't crash).

\item The CIS and the unit test upon which it relies shall include checks for the physics content of the code (e.g. a particular observable being calculated matches the expected vlaue according to set criteria).

\item The CIS and if necessary, individual unit tests, shall be linked to the Issue Tracking System deployed by the Collaboration.

\item Full diagnostic information generated by CIS shall be stored such that it is accessible to the members of the Collaboration.

\item The full retention policy for diagnostic information generated by CIS shall be determined by the S\&C Organization.
\end{itemize}


\subsubsection{Software Distribution and Configuration}
\textit{Description}

\noindent
Software distribution is the mechanism by which the software intended to run on a particular host or facility is being made available for this purpose. This is a non-exclusive list of methods that can be employed for this:

\begin{itemize}
\item Local installation from source code, as described above, resulting in placement of binaries and other components in a local file system accessible to the Worker Nodes, desktops and other computing devices and services at a particular location.

\item Building the binaries from source at a central facility, for all supported platforms, and subsequent managed ``push'' (copy) of the software pre-built in this manner to remote sites.

\item Using network file systems such as CVMFS and others, to make the software accessible to the target computer without having completed local deployment.
\end{itemize}

\noindent
\textit{Requirements}
\begin{itemize}
\item A distribution system shall be created where software is built in a centralized manner and then distributed to remote computing centers (this does not imply that it's the only available or endorsed method).

\item The responsibility for implementation of the distribution system shall be held by the DUNE Software working group.

\item A common configuration mechanism shall exist to ensure proper functioning of the software on each site.

\item Where practical, a mechanism for Worker Nodes to gain accesses to software binaries even in absence of local installation at a particular site shall be provided (i.e. by utilizing network distribution methods whereby binaries can be accessed over WAN).

\end{itemize}

\subsubsection{Use Cases}
\begin{itemize}
\item During a code review, the S\&C reviewers identified hardcoded values for certain geometrical parameters. Findings are documented and later used to rectify the problem by creating an appropriate interface to the geometry database.

\item In the process of a nightly build, a recently added module failed to compile. According to the S\&C policy, the entire release is not considered valid and cannot be used in production.

\item A request is received from one of the users to provide help in installing DUNE Offline Software on a new version of Linux they installed on a desktop system. Since this platform is not officially supported, action is deferred and assistance provided as situation permits on best effort basis.

\end{itemize}

\newpage
\subsection{Distributed Computing}
\label{sec:dunedc}
\textit{Definitions}
\begin{itemize}
	\item \textbf{Grid computing:}  technology that allows a collection of computer resources from multiple locations to reach a common goal.
	
	\item \textbf{Cloud computing:} distributed computing technology with emphasis on network connectivity, virtualization and elasticity of resource allocation.
	
	\item \textbf{Workload Management System (WMS):} a system that enables automated placement of computational payload jobs submitted by its users on distributed resources, using the underlying Grid layer, and makes subsequent record keeping, accounting, elements of data management and general monitoring available to the user.
	
	\item \textbf{Monitoring:} in this context, it represents a system which stores the state of individual jobs, groups of jobs, data transmission and other objects vital for Workload Management, while providing appropriate interfaces to end-users and operators of a WMS.
\end{itemize}

\subsubsection{Grid and Cloud Computing Capabilities}
\noindent
\textit{Description}

\noindent
Described here are basic design principles and approaches in the area of organizing the distributed computing resources being
utilized by the Collaboration. In the past decade, Grid/Cloud and Distributed
Data technology has been embraced by most major scientific projects, for reasons of scalability,
better access to resources and their efficient utilization. DUNE needs to closely examine its
approach to this domain and create  a strategy for integration of Grid and Cloud Computing capabilities
into its infrastructure base.
\ 
\\
\ 
\\
\textit{Issues}
\begin{itemize}
	\item We aim for most efficient utilization of all resources available to the Collaboration, which includes both efficient use of all 
	available hardware, and the ability of all DUNE members to access the resource in a transparent, agile and efficient manner.
	
	\item Hedging against significant uncertainties inherent in estimating and planning resource allocation over the next 
	10 years, against the backdrop of the quickly evolving technology landscape.
	
	\item Grid Sites may have a wide range of capabilities, interfaces and other configuration parameters that need to be accounted for in order for these Sites to be properly interfaced and integrated.
	
	\item DUNE is a large and geographically dispersed Collaboration, therefore there is a need to properly integrate a variety of distributed 
	computing resources in order to optimize utilization of all hardware and personnel.
\end{itemize}
\ 
\\
\textit{Requirements}
\begin{itemize}
	\item A widely distributed computing infrastructure, featuring a network of federated resources (including Grid- and Cloud-based resources) shall be implemented.
	
	\item This widely distributed computing infrastructure shall be put in place by the DUNE S\&C Organization in close cooperation with participating computing sites, institutions and agencies (cf. the Open Science Grid etc).

	
	\item The distributed resources shall include processing facilities, storage, network hubs and their combinations (e.g. Grid sites with large available storage capacity).
	
	\item Necessary tools and procedures shall be provided, for streamlined incorporation of new facilities as they become available, efficient use of opportunistic resources 
	
	\item The effort and expertise across all of the Collaboration shall be leveraged in order to provide adequate operational support with a minimum of manpower.
	
	\item Details of site capabilities, interfaces, configuration and other crucial information will be gathered and preserved as documents and database records, and made available through appropriate information systems (often termed Grid Information System).
\end{itemize}

\subsubsection{Workload Management System}
\label{sec:req-wms}
\textit{Description}

Workload Management Systems serve as a force multiplier for the user of Grid and Cloud technology. They insulate individual users from specific configuration details and certain failure modes of Grid sites and networks, and provide substantial automation in managing the user's computational payload on the Grid. A good WMS also must provide adequate monitoring capabilities (down to the job level) which serve as a valuable debugging tool, and represent an essential toolkit for the operational support teams.

A WMS is also a valuable tool for managing software deployment, keeping proper accounting information regarding releases and document configuration (e.g. release) of the software used for a specific production run, thus satisfying one of core requirements related to processed data in DUNE.

\noindent
\textit{Issues}
\begin{itemize}
	\item Due to a very large number of individual computational payload jobs, wide variations of their type, requirements, environment and other vital characteristics, having access to Grid facilities in itself does not guarantee acceptable degree of efficient utilization and availability of resources, and/or functioning Operational Support.
	
	\item Agility in resource allocation and sufficient monitoring capability is not included in the typical basic Grid middleware stack, while being essential for efficient operation of the experiment.
	\item A robust and user-friendly web information system, allowing individual researchers and working groups to track the execution of 
	their payload submitted to the Grid, and to identify potential issues related to their workflows, will increase overall productivity and help ensure validity of results obtained.
\end{itemize}
\noindent
\textit{Requirements}
\begin{itemize}
	\item DUNE  shall implement a Workload Management System (WMS) for resource management and brokerage functionality 
	which will govern  distribution of most types of computational workload in DUNE (e.g. production jobs, group analysis etc) 
	across variety of resources available to the Collaboration.
	
	\item The DUNE WMS shall be capable of keeping precise record of the software configuration used for each and every
	job deployed on the Grid, including, among other things, the DUNE Offline Software Release information.
	
	\item The DUNE WMS shall be capable of quickly suspending participating sites due to outages, network congestion or potential security issues.
	
	\item The DUNE WMS shall be augmented with a Workflow Management layer, which will help create and manage large groups of Grid tasks supporting the scientific workflows.
	
	\item An DUNE WMS Monitoring System shall be implemented to allow efficient operation of the WMS, by helping ascertain
	correct execution of Grid jobs, accounting of resource utilization, identification and debugging of failure modes etc.
	
	\item The DUNE WMS Monitoring System shall keep status records for individual jobs and their groups, information
	related to data I/O and transmission, and include the ``big picture'' performance monitoring data for entire Grid sites utilized by DUNE.
	
	\item The DUNE WMS Monitoring System shall have interfaces conducive to integration with both Web UI for users and operators,
	and with automated systems needing the WMS data.

\end{itemize}

\subsubsection{Technology trends in Distributed Computing}
\textit{Description}

\noindent
Technology is progressing at a brisk pace and it's virtually impossible to cover its various implications for DUNE Distributed Computing within the scope of this document. We shall itemize, however, a few directions that we'll need to explore in the next few years, such as High-Performance Computing (HPC), multi-core and multi-threaded processing, GPUs etc.
\noindent
\textit{Issues}
\begin{itemize}
	\item High-Performance Computing (HPC) platforms have very different characteristics from the High-Throughput (HTC) systems commonly used in HEP.
	
	\item Multi-core technology has become a standard feature of the hardware utilized at data centers everywhere, and its efficient utilization clearly remains a challenge.
	
	\item GPUs continue to offer exceptional performance in a few application domains, while requiring specialized software development approaches, tools and architectures.
	
\end{itemize}
\noindent
\textit{Requirements}
\begin{itemize}
	\item DUNE shall be proactive in its assessment of multi-core and other multiprocessor technology in order to assure efficient
	resource utilization and leveraging available technologies for better performance of its physics tools software.
	
	\item The S\&C Organization in cooperation with the DUNE Physics Tools Group shall work to explore potential use of GPUs and HPC facilities for DUNE needs.
	
	\item The S\&C TAC shall be tasked with making recommendations for multi-core implementations of DUNE Offline Software
	and specific approaches and standards that must be followed in this area.
	
\end{itemize}

\subsubsection{Use Cases}
\begin{itemize}
	\item  An  DUNE member institution prefers to be able to host and process significant parts of the data,
	and to run some new type of analyses locally for expediency and efficiency, while still retaining the possibility
	of using larger opportunistic resources available to DUNE as a whole, if necessary. This is made possible by
	using the DUNE Workload Management System and its distributed data handling mechanism.
	
	\item  While running managed production at FNAL, it is discovered that the deadline for delivering results
	to an important conference cannot be met assuming only local resources are available. At the same time,
	certain member institutions possess extra computing power that can be used to mitigate the shortage.
	Additional Grid jobs are dispatched, thus filling the need for more resources.
	
	\item A small team of researchers are running an important MC simulation on their very limited local resources,
	and need to accumulate a lot more statistics than can be reasonably expected with what is available to them.
	They take advantage of facilities at FNAL, which is made possible by transparency of access and data handling,
	and user-friendly Grid interfaces.
	
	\item A user observes that a large portion of her jobs started failing according to the monitoring data coming from
	the WMS. Using the job-level monitoring facilities provided by DUNE, she's able to pinpoint and fix the problem.
	
	\item There is a new and intermittent problem with network connectivity to one of the DUNE sites, resulting in
	excessive data transmission times and inefficient resource utilization. This condition is recognized by DUNE
	Operations Team and the site is suspended for WMS purposes while the problem is worked on.
\end{itemize}

\newpage
\subsection{Geometry}
\subsubsection{Description}

Managing geometry description and model presents a challenge to most experiments, and DUNE is no exception.
Ideally, there is ``one source'' geometry description which feeds both Monte Carlo simulations and reconstruction of data,
and potentially takes advantage of interfaces to CAD systems etc. The obvious benefits include assurance of the integrity
of the geometry data, and leveraging visualization systems across various sectors of the experiment. In practice, such complete
integration is hard to achieve. Nevertheless, we identify  a few requirements in this area that should be met.

\subsubsection{Definitions}
\begin{itemize}
	\item  \textbf{Geometry description:} a collection of information sufficient for creation of the geometry model of the detector,
	for the purposes of either simulation, reconstruction or both.

	\item  \textbf{Geometry model:} a collection of data structures plus the code to manipulate these structures,
	which together provide the functionality required by the application (simulation etc, cf. the geometry model in Geant4).
\end{itemize}

\subsubsection{Issues}
\begin{itemize}
	
	\item  There is a variety of domains utilizing geometry data, such as simulation, reconstruction, packages specific to the Far and Near Detectors, beamline description, overburden maps, interaction packages.
	
	\item  Legacy code and ad hoc solutions are already utilized in these work areas.
	
	\item There are compelling advantages of a single, unified system, such as referential integrity of geometrical data.
	
	\item Placing geometry under proper revision control reduces possibilities of errors when studying various configurations 
	or making improvements in geometry description.
	
	\item  Use of object replicas and symmetry exploitation in the geometry description and model in simulations is 
	contrasted with the ``real'' geometry where infividual objects have alignement properties which may effectively 
	break the symmetry -- this is relevant mostly for reconstruction code.
	
	\item Proper use of the geometry model and description depends on validation mechanisms: visual, overlaps detection, tracking tests.
	
	\item One must consider interfaces that need to be built to existing and future tracking and visualization packages.
	
\end{itemize}

\subsubsection{Requirements}
\begin{itemize}
	\item DUNE S\&C Organization shall develop a  geometry data management system from which specific applications may derive their geometry information.
	
	\item The Geometry Description formats and interfaces shall be chosen in a way that is not restricted to a single 
	software package or toolkit when building the Geometry Model.
	
	\item Provision shall be made to accomodate non-ideal, ``distorted'' or ``perturbed'' versions of geometry description, 
	containing the realistic alignment parameters (e.g. as determined by a survey \textit{in situ}).
	
	\item The geometry data management system shall support reliable storage and  revision control of any set of geometry information.
	
\end{itemize}

\newpage
\subsection{Visualization}
\subsubsection{Description}

This section describes requirements related to visualization tools used to produce graphics as required by DUNE. CAD and other engineering systems are not included here. Visualization in this context is the process of displaying both the structure of various components of the DUNE apparatus, and optionally also the event data. A special case of combined visual representation of both is often referred to as ``event display''.

\subsubsection{Issues}
\begin{itemize}
	
	\item Visualization tools are important for many types of research, development and data analysis work in DUNE.
	
	\item There is a variety of legacy geometry descriptions, split across the detector subsystems, and variety of models that need to be interfaced.
	
	\item The Liquid Argon TPC is a precision tracking apparatus and by its very nature a ``visual'' detector, and 
	its capabilities can be leveraged by having robust and versatile ways to display the event data.
	
	\item The role of visualization tools in the development of tracking algorithms of DUNE is especially important.
\end{itemize}

\subsubsection{Requirements}
\begin{itemize}
	\item Applications and supporting tools to visualize the DUNE geometry and event data shall be provided by the S\&C Organization.
	
	\item A visualization toolkit available to DUNE shall be created.
	
	\item The visualization toolkit shall cover a range of functionalities, including interactive event displays, and capability to produce publication-grade graphics.
	
	\item Specific needs of simulation work shall be accommodated, such as displaying ``Monte Carlo truth'' information.
	
	\item The visualization toolkit shall cover the needs of expert users (such as during event scanning studies) as well as general needs of the collaborators who do not necessarily possess significant programming expertise.
	
\end{itemize}

\subsection{Networks}

\subsubsection{Description}
The network infrastructure of DUNE is important for two main reasons. One, â€œtraditionalâ€ reason is that DUNE will depend on the networks for distribution of  data to storage facilities and computing centers located at its member institutions, as well as for distribution and management of its computational workload on the Grid. This will require a focused effort necessary for design, deployment and continuous monitoring and maintenance of the DUNE networking infrastructure.

The other and somewhat more unusual reason  is due to the fact that the Far Detector is situated at a remote location and indeed very far from the target and the beam infrastructure,  and importantly the storage facility at FNAL. Thus the network link to the remote site, carrying information like ``beam on target'' is of vital importance to DUNE, and so is a reliable and high bandwidth network connection to transmit data from the Far Detector to the principal storage facility.

\subsubsection{Far Site Connectivity}
\textit{Issues}

\begin{itemize}
	
	\item ``Beam on target'' network signal containing timing information about the beam will be used by the online systems.
	
	\item Raw data will be transmitted from the Far Detector (FD) to the principal storage facility tentatively located at FNAL, which also ties into the topic of buffering the data at the FD location to compensate for occasional problems with the network connectivity issues and performance.
	
\end{itemize}
\noindent
\textit{Requirements}
\begin{itemize}
	\item A R\&D effort shall be undertaken to design and create the necessary infrastructure, including software and requisite systems integration, 
	for the ``Beam on Target'' signal.
	
	\item Research to establish performance parameters for the primary data network connection between the Far Detector and  DUNE data storage centers (e.g. FNAL) shall be conducted by the S\&C Organization.
	
\end{itemize}

\subsubsection{DUNE Data Network}
\textit{Issues}
\begin{itemize}
	\item DUNE will utilize a global and complex network to distribute, collect and otherwise manage its data across multiple data centers around the globe.
	
	\item Individual elements of the networks utilized by DUNE will have vastly varying performance and reliability characteristics, subject to change in time.
	
	\item In order to use available resources in an efficient manner, DUNE will need to be agile and adaptive in its utilization of networks.
\end{itemize}
\noindent
\textit{Requirements}
\begin{itemize}
	
	\item A network monitoring system shall be put in place to continuously gauge the bandwidth, 
	availability and other vital parameters of network connections established among participating DUNE sites.
	
	\item Network metrics shall be collected and stored in a database for analysis, and used for debugging and optimizing data distribution algorithms and policies.
	
	\item The network monitoring system and its associated databases  shall be equipped with appropriate interfaces to make the monitoring and performance data available to DUNE data distribution and Workload Management systems .
	
\end{itemize}
\noindent
\textit{Justifications}

\begin{itemize}
	
	\item Site-to-site network performance does not always stay constant. Ignoring the current network characteristics 
	may result in suboptimal distribution of computational payload among participation sites, resulting in longer time to task 
	completion.
	
	\item Having a network monitoring system is helpful in detecting and reporting outright failures.
	
\end{itemize}
\noindent
\textit{Use Cases}
\begin{itemize}
	\item A data processing campaign is under way, and input data needs to be shipped to a few sites. 
	The monitoring system deployed by DUNE collects network performance metrics that indicate that it
	won't be optimal to transmit data to a few sites due to insufficient bandwidth, and these sites are 
	subsequently used mainly for Monte Carlo studies. At a later point, when the network problems are resolved, primary data transmission is resumed.  The net result is better efficiency of the overall system.
	
	\item A sudden connectivity glitch between two remote sites is detected by an automated system,
	and a  notification is sent to DUNE Operations Support.
\end{itemize}
\noindent
\textit{Alternatives}
\noindent
In case a centralized network monitoring system is not created, monitoring the network
functionality and performance can  be delegated to personnel located at individual sites.

%++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\newpage
\subsection{Cybersecurity}
\subsubsection{Description}
Attacks on scientific computing facilities by malevolent agents are a reality that needs to be countered. Due to users being lax in fully implementing security protocols, improper site configuration and other factors, these attacks are successful at times.

\subsubsection{Issues}
\begin{itemize}
	\item It is impossible to prevent malevolent entities from attacking the DUNE computing facilities, such attacks are commonplace.
	
	\item There are proven methodologies, policies and approaches as well as rules put in place by the agencies, that minimize the success rate of such attacks, and these policies and rules must be fully followed.
\end{itemize}

\subsubsection{Requirements}
\begin{itemize}
	\item DUNE S\&C shall establish close cooperation with Cybersecurity personnel at participating sites.
	
	\item Participating sites shall be required to report all DUNE security related incidents to S\&C on a monthly basis or more frequently in a crisis situation.
	
	\item All rules and policies set forth by DOE and other authoritative sources shall be implemented.
	
	\item Protocols shall be established to stop and mitigate attacks.
	
	\item The DUNE S\&C Organization shall periodically review the configuration and security features of its member sites.
	
	\item Creation and use of ``production accounts'', i.e. user identities associated with automated services, agents and other such type of computer processes, shall be kept to absolute minimum, reviewed on case-by-case basis and subject to strict information protection.
	
	\item A blacklisting capability in the DUNE Workload Management System shall be created which will make it possible to block compromised or abused accounts on short notice.
	
	\item Blacklisting capability in additional DUNE Offline Software systems shall be determined necessary by the DUNE S\&C Organization.
	
\end{itemize}

\subsubsection{Use Cases}
\begin{itemize}
	\item Unusual activity is detected on one of DUNE distributed sites. Local personnel determines that this is a case of a hijacked account used to mount a DOS attack. Corresponding processes are located and eliminated, while the account in question is blacklisted until further investigation and corrective measures.
	
	\item DUNE interacts with organizations like the Open Science Grid in order to conduct an internal review of its Cybersecurity protocols and implement best practices going forward.
\end{itemize}

