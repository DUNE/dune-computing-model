%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%  The 35t COMPUTING MODEL. Start of work November 2017, separating %%%%%%%%%%
%% this material from the main DUNE COMPUTING MODEL which previous contains this as an appendix    %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}

\topmargin=-0.5in
\oddsidemargin=0in
\evensidemargin=0in
\textwidth=6.2in
\textheight=9.25in

\usepackage[dvips]{graphics}
\usepackage[nomargin,inline,marginclue,draft]{fixme}
\usepackage{rotating}
\usepackage{amssymb,amsmath}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{enumerate}

%\usepackage[titletoc]{appendix}

\usepackage{multirow}

%% draftwatermark causes infinite loop on SL6's ancient TeXLive
% \usepackage[firstpage]{draftwatermark}
\usepackage{lipsum}
\usepackage{tikz}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[blocks,affil-it]{authblk}

\usepackage{xspace}


\newcommand{\pd}{protoDUNE\xspace}
\newcommand{\filesize}{5\,GB\xspace}

% experimental - for diameter symbol etc
\usepackage{wasysym}

\RequirePackage{lineno}

\bibliographystyle{plain}
\bibliographystyle{unsrt}

\usepackage{graphicx, subfigure}
\usepackage[colorlinks=true,urlcolor=black,linkcolor=black,citecolor=black,bookmarks=true]{hyperref}

% Depth can be adjusted, and 3 may seem excessive, but when set to 2, a lot of interesting detail in TOC is lost.
\setcounter{tocdepth}{3}


\begin{document}

%\linenumbers

\title{Computing Model for the 35t Prototype of the DUNE Experiment}

\date{\today}

\maketitle

\centerline{The DUNE Collaboration}


%\SetWatermarkText{DRAFT}
%\SetWatermarkLightness{0.9}
%\SetWatermarkScale{3}

\vspace{2cm}


\begin{abstract}

\noindent  The 35t prototype built and operated by the DUNE
Collaboration was an important milestone in the development
of the DUNE Far Detector. The measurements were completed
in early 2016 and the analysis phase was mostly completed
in late 2017. This document aims to systemize and preserve useful
information about the computing model of this prototype program.
\end{abstract}



% --- executive summary section ---
\newpage
%\centerline{\textbf{\centerline{\bf{\Large {Executive Summary}}}}} 
%\input{execsum}

%++++++++++++++++++++++++++++++++++++++++
% TOC and lists of tables and figures
\newpage
%\tableofcontents
%\newpage
%\listoftables
%\newpage
%\listoffigures


%++++++++++++ MAIN DOCUMENT +++++++++++++++
\newpage

\section{Overview}
The 35-ton prototype of the DUNE single-phase LArTPC had two operating periods, called Phase\,I and Phase\,II.
Operations ended in early 2016.
\begin{itemize}

\item Phase\,I, which took place in 2014, was designed to demonstrate that a sufficiently high electron
lifetime is achievable in a non-evacuated membrane cryostat. A lifetime of $\sim$3\,ms was achieved.

\item For Phase\,II, TPC elements were installed, and a Data Acqusition system developed and deployed. 
The detector is segmented into four two-sided APA's for eight drift volumes. The total channel count
consists of 2048 TPC wires, 92 photon detector channels reading out six independent photon
detection devices, and scintillator paddles arranged outside of the cryostat.

\end{itemize}

\noindent
Phase\,II concluded in 2016 and included continued demonstration of an adequate electron
lifetime with TPC hardware present in the cryostat, and characterization of the detector performance.
The unique elements of the DUNE Far Detector single-phase design which are replicated in this prototype are:
\begin{itemize}
\item  modular TPC elements (multiple APA's), and wrapped induction-plane wires
\item photon detectors mounted inside the APA frames
\item two-sided APAs reading opposing drift volumes, with gaps between the APAs
\item cold electronics similar to the Far Detector's design
\end{itemize}

\noindent
The 35-ton prototype detector was not to designed be used with a test beam, and instead relied on cosmic rays as
a particle source used to characterize the detector.  Measurement and analysis topics included the determination of:
\begin{itemize}
\item the electron drift velocity and lifetime
\item the signal and noise characteristics of the TPC as well of the photon detectors
\item distortions due to buildup of space charge in the TPC volume
\end{itemize}

\noindent
External scintillator trigger paddles served to identify samples of cosmic rays that are particularly useful in determining
detector characteristics.

Approximately 200\,TB of raw data has been collected by the 35-ton prototype in Phase\,II, not including processed versions of the data.
The sections below describe the computing model utilized to collect and analyze these data.

\section{Online Computing Model for the 35t Prototype}

Custom cold electronics attached to the TPC wires amplify the incoming signal, shape it in time,
and digitize it.  These data are sent to sixteen Recofigurable Cluster Elements (RCEs), each of which
processes the data from 128 channels.  The RCEs format the data, apply trigger decisions, and send the
data along ethernet connections to the DAQ computers.  The RCEs perform zero suppression for
much of the data to be collected.

Data from the photon detectors are assembled by SSP (Silicon Photomultiplier Signal Processor),
and sent from there via ethernet connections to the DAQ computers.  The triggering is implemented
in the so-called ``Penn Board'', which inputs signals from the scintillator paddles.  Data
from the Penn Board is also included in the event data stream.

There are seven DAQ server nodes, which are connected to each other via a private network, as well as two gateway nodes. 
These two gateway nodes are also connected via a 1\,Gbps link to the FNAL campus network. The DAQ nodes run {\it artdaq},
a distributed application for acquiring, assembling and storing data from high-energy physics experiments. 

Two of the computers near the detector each contain 10\,TB of buffer disk.  This disk is used to store the first copy of
the data and monitoring histograms before being transferred to permanent storage. The data is copied
to one of the gateway computers located in the same rack as the DAQ computers.  The gateway computer
extracts metadata from the DAQ files and also reads the online configuration database, and 
metadata registeratopm takes place with Fermilab's  SAM system (``Sequential Access via Metadata'').
Then the data are moved to permanent storage utilizing the ``dropbox'' functionality of the File Transfer Service at FNAL (F-FTS).
The gateway computer also runs near-line monitoring processes and an event display.
When the data have been stored in persistent storage and the files are no longer needed online,
the copies on the DAQ and gateway node can be retired, which is done after validation by checksum and
total byte count comparison.

Online monitoring processing is included in the {\it artdaq} suite, with experiment-specific modules
provided by the DUNE 35-ton collaboration.  Histograms are stored as root files and also
images for display on web servers are produced at the end of a run, and are backed up
to a permanent storage.


\section{Offline Computing Model 35t Prototype}

Once the data are copied to one of the dropboxes, the F-FTS (data management system developed at FNAL)
copies them to tape-backed locations in dCache,
a large disk pool used to increase the efficiency of access to large datasets on tape.  Enstore is Fermilab's
tape-handling system.  Files are requested using SAM, which issues commands for reading data off of tape
and into dCache, where they can be then transferred to batch worker nodes or read directly.  
The online database is replicated offline, and the offline replica handles queries.  A conditions database
stores calibration parameters such as pedestals and gains.

%Offline jobs first unpack the data and then split the data into events that are convenient to analyze.
%Depending on the run mode, the data records produced by the DAQ may correspond to as many as ten
%drift times or longer, while offline reconstruction and even event visualization is more convenient with
%smaller slices of time.  Events then are defined by the offline event-slicer, whose behavior is configurable
%depending on the type of events sought.  DAQ records in adjacent time slices may be ``stitched'' together
%to form offline events that span DAQ records.  Standard LArSoft data calibration and reconstruction tools
%as described in the DUNE CDR will then run on the data, producing both reconstructed event data and an
%analysis tree consisting of high-level objects which are easy to interpret by analysis teams measuring
%the performance of the detector.

Some runs (in the ``scope mode'') collected data that is not well suited for LArSoft to analyze.  A small subset
of channels was read out continously for a very long time, of order seconds, in order to measure
low-frequency noise and look for electronics effects that may take longer than a normal readout window.

The primary data store is the  FNAL Enstore tape system, with access via SAM and tape staging in
dCache.  Directly reading tapes by jobs is not permitted.  Data are transferred to local disk on grid
worker nodes using standard FNAL tools.  Data in dCache will also be available via XRootD and other protocols.

The primary reconstruction is done using FermiGrid computing resources.  The capability
of using OSG compute nodes is planned.  The DUNE simulation and reconstruction software, along with the
dependencies LArSoft, root, GEANT4, and other packages, are all copied to CVMFS repositories for use on
OSG nodes.  If the CPU requirements become large and urgent, then the tools are in place to start using
OSG CPU, although some operational debugging and testing is still required for this avenue to be in smooth
production.  Fermigrid monitoring is provided by FIFEMON, and OSG accounting by GRATIA.

The data will be reconstructed using LArSoft, which contains general-purpose LArTPC reconstruction algorithms,
as well as DUNE-specific pieces, which mainly address the specific issues of geometry and data formatting.
In addition to distributing pre-built code on CVMFS, both pre-built libraries and source-code distributions
are available via {\tt http://scisoft.fnal.gov}.


\section{Resource Requirements}
\label{sec:35t-resource-requirements}
The amount of data collected by the 35-ton Phase\,II run is approximately 200\,TB. 
%The amount of raw data collected
%depends strongly on the running time, the run plan, the zero-suppression strategy, and the
%compressibility of the data.  Noise sources, unstable pedestals, and conservative choices for zero-suppression
%thresholds and leading and trailing sampling times will increase the data volume.
%The expected volume of raw data to be stored on tape is approximately 100 TBytes, though as much as a factor
%of four may be stored depending on the abovementioned unknowns.
The reconstruction of each event takes less than one minute of CPU time.  The rate at which
a CPU core can process data has not evolved much in the last seven years.  An estimate of approximately
4\,Million CPU hours per year is needed to reconstruct and analyze 35-ton data.  Storage adequate for
two versions of reconstruction, which contain copies of the raw data, should be available at any given time.
Simulation samples are expected to be of a similar size or smaller than the data sample, as many anaysis
topics will be data-driven in order to characterize detector performance.
\end{document}
