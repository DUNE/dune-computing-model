\section{DUNE Computing Model}
\label{sec:computing_model}

\subsection{Introduction}
The main purpose of the Computing Model is to guide the creation of DUNE computing fabric, spanning
hardware, middleware and various software components. Such fabric would give to all members of the DUNE
and to raw data for calibration and any other activities as required by the scientific objectives of the experiment.
To meet these goals, and in fulfillment of the data access requirements (\ref{sec:rules-of-access-to-data})
the model presented here is based on distributed data management and Grid and Cloud Computing concepts.

As stated in \ref{sec:modelrole}, the Computing Model is built on the basis of two supporting documents: the
``Software and Computing Requirements'', which sets forth policies and practices for the DUNE computing sector
(Section\,\ref{sec:requirements}), and ``DUNE Data Characteristics'' (Section\,\ref{sec:data-characteristics}) which contains information
necessary for quantifying parameters of the Computing Model. Both documents will be referenced extensively in this section.

Unless stated, no final technology choices are made in this section -- due to a long period of time before the DUNE commissioning,
this will need to be done at a later date and in compliance with the ``Requirements''. Where appropriate, existing systems are described and
considered for use in DUNE.

\subsection{Data rate and volume estimates}

\subsubsection{Overview}
Rate and volume of the various data to be produced in DUNE are perhaps the main driver determining the parameters
of the Computing Model. Data rates for different types and their relationships were considered in Section~\ref{sec:data-characteristics},
and the material below will present a summary and aggregated estimates for the scale of the data of various types
and origin.

It is important to keep in mind that estimates presented in Section~\ref{sec:data-characteristics} were developed under ``baseline''
assumptions necessary for groundwork done in the process of the Conceptual Design Report creation. As DUNE is engaged in R\&D aimed at
optimal design of the detector and the physics tools, it is likely that these assumptions will be revisited at some point and
the data rate and volume estimates will need to be revised correspondingly.

\subsubsection{Raw Data}
Characteristics of the data to be produced by DUNE detector systems have been considered in
Sec.~\ref{sec:data-characteristics}.

At the time of writing estimations for the Far Detector data are better understood when compared to other DUNE subsystems,
due to better developed geometry and parameters of the LArTPC, technology and configuration choices, and considerations
presented in subsections~\ref{sec:daq-assumptions} (notes on DAQ),~\ref{sec:zs-data} (Zero Suppression) and \ref{sec:data-compression} (Compression).

A distinguishing characteristic of the data flow in DUNE Far Detector is a significant data reduction factor to be achieved
in the Far Detector DAQ, as evidenced by the numbers presented in Tables~\ref{tab:full-stream-volume} and \ref{tab:zs-volume}.
The key assumption in this is the capability of DAQ to distinguish low energy signals  due to $^{39}$Ar decays spread
across the volume of the detector from more interesting physics phenomena (see~\ref{sec:ar39decays}).
This and similar capabilities will be provided
by deploying appropriate algorithms on the DAQ RCE and online farm utilizing about a hundred computers (estimated).
Estimates of volume of data due to high-energy interaction (cosmic $\mu$ and beam neutrinos) are presented in
Table~\ref{tab:fd-data-volume-summary} (page \pageref{tab:fd-data-volume-summary}) and are quite modest.

Processing required to identify candidate SNB and nucleon decay events will also be handled by the DAQ system and its online farm.
As discussed in subsection \ref{sec:snb-data}, once a SNB trigger condition has been established, the data must
be recorded without zero suppression in order to capture the characteristically low energy and scattered ionization
clusters in the detector volume as expected in a supernova burst event. As shown in Table~\ref{tab:zs-volume} (page~\pageref{tab:zs-volume}),
the resulting volume of data is significant and is currently estimated as just over half a PB annually, given the premise of accepting roughly
12 false positives per year (at this point a benchmark number used solely to set the scale of the data, as commented in~\ref{sec:snb-data}).

Identifying candidate nucleon decay events will also require algorithms that would set the trigger condition consistent
with signatures such as $p \rightarrow K^+\nu$. As shown in \ref{sec:pdk-data}, the volume of data due to such candidate
events is not expected to be appreciable compared to other sources.

The Near Detector is quite different from the Far Detector in all of its characteristics, and it includes a number of subdetectors
such as Fine-Grained Tracker, Electromagnetic Calorimeter and others (see~\ref{sec:nds-event-rates}). There are a few technology
options being conisdered and most parameters are still being finalized at the time of writing, so data estimates are very preliminary.
It is
%conservatively
assumed that the annual volume of these data will be 100\,TB.

%Current numbers result in annual volume from a few dozen to a hundred TB.

%In summary, it is anticipated that under assumptions presented above the total volume of data in DUNE to be
%committed to storage will be up to 1PB per year of data taking.

\subsubsection{Monte Carlo Data}
As discussed in \ref{sec:mc-data-estimates}, the Far and Near Detector simulation is the top contributor to the volume of overall
Monte Carlo in DUNE, with combined annual output of about 35\,TB of data. Other MC data sources are not appreciable.
It is reasonable to expect that this number will grow as more effort becomes available for Monte Carlo studies and
as they become more sophisicated. However, at the time of writing there is no reliable estimate of how this segment of data
will scale over the years.


\subsubsection{Summary of Annual Data Volume Estimates}
%\subsubsection{Summary Table}
Table~\ref{tab:summary-data-table} contains a summary of estimations for various sources
of data in DUNE, in terms of projected annual volume, which is based on information developed in
this document up to this point. Where applicable, entries are marked with ZS (Zero-Suppression) or
FS (Full-Stream). Atmospheric $\nu$ were not included as discussed in \ref{sec:atmo-nu}.
\begin{table}[ht!]
	\centering
	\begin{tabular}{| p{1.2in}| p{1.7in} | p{1.5in} | p{0.65in} |}
		\hline
		\textbf{Data Type} & \textbf{Source} & \textbf{ZS/FS} & \textbf{Volume} \\ \hline
		Raw & cosmic-$\mu$  & ZS$^*$ \textit{(see comments)}& 4.8\,TB \\	\hline
		Raw & beam-$\nu$  & ZS$^*$ \textit{(see comments)}& 5.3\,GB  \\	\hline
		Raw & Nucleon Decay Cand.  & ZS$^*$ \textit{(see comments)}& 480\,GB  \\	\hline
		Raw & SNB candidate & FS & 553\,TB \\	\hline \hline \hline
		Raw & \textbf{Total Raw} & - & \textbf{558\,TB} \\		\hline \hline \hline
		MC & Beamline and Target  & - & 1\,TB \\	\hline
		MC & Far Detector & ZS$^{**}$  \textit{(see comments)}  & 15\,TB \\		\hline
		MC & Near Detector & - &20\,TB \\ \hline \hline \hline
		MC & \textbf{Total MC} & - & \textbf{36\,TB} \\		\hline \hline \hline
		Derived &  Raw  & - & 4.5\,PB \\	\hline
		Derived &  MC  & - & 288\,TB \\	\hline  \hline \hline
		Derived & \textbf{Total Derived} & - & \textbf{4.8\,PB} \\		\hline \hline \hline
		All & \textbf{Grand Total} & - & \textbf{5.4\,PB} \\		\hline % \hline \hline
	\end{tabular}
	\caption{Summary of annual data volume estimates due to various sources.}
	\label{tab:summary-data-table}
\end{table}
Comments to the table:
\begin{itemize}
\item ($^*$) Estimates for the raw data ($\nu$ and $\mu$ but not Supernova canidates) assume ``smart'' Zero Suppression, i.e. extended
window around the peak of the pulse where data is preserved regardless of its instantaneous value (see \ref{sec:zs-data} on page~\pageref{sec:zs-data})

\item ($^{**}$) As discussed in~\ref{sec:data-compression}, at present the zero-suppression algorithms employed in production of the
MC data are not very efficient, so there is potential for further data reduction

\item Entries for the Derived Data -- ESD/AOD --  include the ``headroom factor'' of 4 (see justification in \ref{sec:derived-data})
\end{itemize}

\subsubsection{Implications of the Estimates and Possible Optimization of Data Collection Parameters}
The items summarized in Table~\ref{tab:summary-data-table} have been prepared under a set of specific
assumptions as documented in corresponding subsections of Section~\ref{sec:data-characteristics}.

Even without revisiting much detail, salient features the data volume distribution across various sources are
obvious. The SNB data are the largest source of data by a large margin. This is due to a combination of factors such as:
\begin{itemize}

\item The requirement that SNB candidate data in general should not be discarded (see arguments in \ref{sec:snb-data}).

\item The requirement of high sensitivity of DUNE to the Supernova Burst candidate events, which provides motivation
for the trigger logic to be set at high sensitivity (hence the declared number of expected false positives, which is nominal
and is mainly included to set the scale).

\item The requirement that the SNB candidate events are recorded essentially without Zero-Suppression to ensure that low energy
clusters of ionization spread in the detector volume are captured with proper efficiency.

\item The requirement that the digitized data is being read out from the Far Detector LArTPC and recorded continuously over
the nominal window of 10\,s (longer recording periods may in fact be desirable).

\item Huge total number of channels in the Far Detector (over 1.5\,million).

\end{itemize}

\noindent
It is also obvious from Table~\ref{tab:summary-data-table} that the very events that constitute the main science objective of DUNE
out of a few -- the beam $\nu$ events -- produce a tiny nominal amount of data when compared to other sources. From the standpoint
of resource utilization, this may or may not be optimal and further guidance will be needed from the Collaboration.

It may become helpful then to consider adjustments to the parameters and approaches used in the criteria for data collection, such as
thresholds, ZS algorithms, SNB candidate retention policies etc. The most obvious change that may be done is to make zero suppression
for the beam $\nu$ substantially more lax. While it will take a careful study to quantify the effects of various levels of zero suppression
on the physics performance of the overall system and analysis chain, quanlitatively its clear that lowering the ZS thresholds and/or
extending the recording window around the peak of the signal has the potential to provide more flexibility in event processing (since
more can be done offline without loss of information) and improve the resolution of the charge measurement. Such study appears to
be useful in working on the next iteration of the computing model.

% Another issue to address is the SNB candidate retention policies. Proper statistical analysis is required to better quanti

\subsection{Data Flow}
\subsubsection{Far Detector Data}
The following description applies to the data attributed to beam neutrinos and to the cosmic ray muons. Due to the vastly
different scale and rate of occurance of the Supernova Burst candidate events, it is not optimal to include this type data in the same
data stream with $\nu$ and $\mu$.

\ 
\\
\noindent
\textit{High-Level View of Data Flow} 
\ 
\\

\noindent
A high-level conceptual diagram of the Far Detector data flow in DUNE in presented in Fig.\ref{fig:DUNEdataflow}
(page\,\pageref{fig:DUNEdataflow}).
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{DUNEdataflow.png}
\caption{Conceptual Diagram of the Data Flow in DUNE.}
\label{fig:DUNEdataflow}
\end{figure}
The top section of the diagram (labeled ``A'') represents systems and data flow at the Far Site.
The mid-section ``B'' depicts the DUNE data storage network. FNAL is the main data center
hosting tape storage for the primary copy of all data, as well as distributed disk (such as dCache).
It is expected that the Metadata system will also be deployed and operated at FNAL.
Additional Grid sites at participating institutions will have replicas of the data.
Replication of the data will be done as per requirements formulated in \ref{sec:req-raw-data-replication}.
\ 
\\

\noindent
\textit{Data Buffering at the Far Site} 
\ 
\\


\noindent
Data Acquisition systems will be located in the specially built rooms in the detector vicinity,
i.e. in the cavern deep underground (commonly referred to as 4850L). The undergound location is to be
connected to surface by 96-strand fiber optic cable, but this does not imply that all strands will be
utilized at any given time. The scale of bandwidth of an individual fiber is 10gbps. It can be therefore
expected that there will be ample headroom for data transmission to the surface under a variety of scenarios.

There will be two layers of data buffering: a buffer in DAQ before the data is transmitted to the surface,
and at the surface facility before the data is transmitted to FNAL which is the primary storage site and the
keeper of the custodial copy of the data.

It is a common practice to provide buffer space for the experiment which is sufficient for intermediate storage of data
for one or maybe a few days in order to keep running even in case of network equipment outages. Given the estimates
presented above, the decisive factor for determining the buffer size will be the data scale of candidate SNB events since
they are likely to be read out at full-stream rate (with lax or no zero-suppression) (see Table~\ref{tab:zs-volume}). The
size of each buffer (the online farm and the surface facility as drawn in section ``A'' of Fig.\ref{fig:DUNEdataflow})
can therefore be estimated as $\sim$50\,TB.
\ 
\\

\noindent
\textit{Data Merging at the Far Site} 
\ 
\\


\noindent
There are additional considerations due to the modular structure of the DUNE Far Detector
which is conceived as four individual TPC modules. In order have to have more flexibility and to minimize down time
and the number of potential single points of failure the DAQ will be segmented into four individual  systems each collecting
data from their respective module~(see \ref{sec:daq-architecture}). To ensure
consistency and simplicity of processing, it is desirable to merge the data streams coming from individual
detectors' DAQ systems at some point so that data is written to files contains readout for the full 4-module
DUNE detector. The surface facility is the optimal location for the merging to take place since it needs to be
equipped with storage and networking equipment for buffering purposes in any scenario.
\ 
\\

\noindent
\textit{Data Transmission from the Far Site} 
\ 
\\

\noindent
Two horizontal arrows in section ``A'' of the diagram in Fig.\ref{fig:DUNEdataflow} represent the WAN connection of
the Far Site to FNAL. Looking at the raw data section of Table~\ref{tab:summary-data-table}, one can estimate
the sustained bandwidth required to transmit all data produced under current set of assumptions as 20\,MB/s (once
again, transmission of buffered SNB candidate data would take most of this bandwidth while items like beam $\nu$
will not occupy an appreciable portion of the bandwidth).

\subsubsection{Data Handling at the Near Site}

The Near Site is located within the boundaries of Fermilab and therefore will have a LAN connection to mass storage
and other central facilities. Raw data will be buffered in the NDS DAQ. Data transmission will be fully automatic and monitored.
Metadata will be managed under the same umbrella as the Far Detector data.

\subsubsection{Interfaces to FNAL Storage Infrastructure}
Both the Far Detector and Near Detector Systems will need to interface storage infrastructure at FNAL.
While technology options and choices may be very different at the time of DUNE commissioning from what they
are at the time of writing, it is instructuve to review what exists at Fermilab at present, as guidance.
Elements of this infrastructure relevant to this dicscussion are:
\begin{itemize}
\item Enstore: tape drive and tape management system.

\item SAM: ``Sequential data Access via Meta-data'', a file based data management and access layer between the Storage Management System
and the data processing layers, developed and maintained at FNAL~\cite{sam_chep12}.

\item dCache: a FNAL instance of a massive distributed storage management system, featuring a virtual filesystem tree with a variety of standard access methods.

\item BlueArc: Mass Storage, disk-based.

\end{itemize}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{fnal-data-infrastructure-sketch-1.png}
\caption{Conceptual Diagram of FD and NF interface to storage infrastructure at FNAL.}
\label{fig:fnal-data-infrastructure}
\end{figure}

\subsection{Data Management}
\subsubsection{LHC Experience with Data Distribution}

In formulating the architecture for data distribution in DUNE, it is helpful to study the experience of the LHC experiments,
since DUNE shares much of the same challenges: potentially significant  volume of data to be collected and processed,
projected long period of operation of the experiment, and geographically dispersed and diverse research communities joining the Collaboration.

When data management systems for the LHC experiments were designed and deployed around the turn of the century, it had to be done
against the backdrop of lack of universal access to high bandwidth networks across collaborations and/or substantial cost of such access~\cite{monarc},
along with reliability issues.
In order to maximize utilization of intellectual and technical resources spread around the globe, a model was adopted in which the computing resources
were organized as a hierarchy of Regional Centers, which allowed for tight control and optimization of data transmission. In this hierarchy, the following
levels were identified:
\begin{itemize}
\item Tier-0: CERN
\item Tier-1: National Center (``large scale and expensive facility'')
\item Tier-2: Regional Center (``smaller, mostly for analysis, local center of expertise and maintenance'')
\item Tier-3: Workgroup Servers at participating institutions
\item Tier-4: Individual computers (desktops)
\end{itemize}
\noindent
In this approach, distribution of data and access to it within an experiment is likewise hierarchical, as shown in Fig.\ref{fig:monarc}.
\begin{figure}[h!]
\centering
\includegraphics[width=0.6\textwidth]{monarc-model.png}
\caption{MONARC model of data distribution.}
\label{fig:monarc}
\end{figure}
Tier-1 centers have assured high bandwidth to CERN (Tier-0) which is typically achieved by establishing a dedicated
optical fiber connection with properly equipped endpoints. Tier-1 facilities form the the basis of further distribution of data
as required, in particular serving data to Tier-2.

Tier-2 centers would not be typically engaged in large scale production activities involving raw data and thus
have reduced network requirements. Smaller Tier-3 facilities would access data via Tier-2 etc.

We observe that at present, these experiments have actually revised the approaches to Data Handling and Workload
Management adopted in preparation for LHC Run-1~\cite{lhc_model_update}.
\begin{figure}[h!]
\centering
\includegraphics[width=0.6\textwidth]{un-monarc-model.png}
\caption{Evolved LHC model of data distribution.}
\label{fig:un-monarc}
\end{figure}
For Data Handling, there is a marked transition from the strictly hierarchical
“tiered” model of data distribution and access (e.g. MONARC) to more flat, agile, scalable and more peer-to-peer-like
architectures (see Fig.~\ref{fig:un-monarc}).
As stated in~\cite{lhc_model_update}
\begin{quotation}
\textit{With the technological progress of wide area networks and consequently improved network connectivity ATLAS has
already in Run-1 gradually moved away from the hierarchical association of Tier-2s to one Tier-1 (MONARC model)
and is now able to associate workflows between a well-connected Tier-2 and several Tier-1s, based on monitored
connectivity metrics. This functionality will be further pursued in Run-2 and is being incorporated into the production
system and distributed data management developments.}
\end{quotation}

\noindent
For Workload Management, the LHC computing models have evolved to put emphasis on access to heterogeneous and sometimes
opportunistic resources, facilitated by dynamic software provisioning. Other challenges being addressed in LHC computing include
the design and handling of Metadata, and managing computational workflows.


\subsubsection{Distributed Data Management}

\subsection{DUNE Analysis}
\subsubsection{Analysis Procedures and Data Flow}
\subsubsection{The Analysis Model}
\subsubsection{Distributed Analysis}

\subsection{Simulation}
\subsection{Calibration}
